{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruparee/rag-pipeline-tutorial-notebook/blob/main/rag-pipeline-tutorial-notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkLcdwcY3z_K"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ruparee/rag-pipeline-tutorial.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74uxJBpp35J8"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_community langchain_pinecone langchain_openai unstructured langchainhub langchain_text_splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3d9y9Bw38gN"
      },
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQFT-ACaKf92"
      },
      "outputs": [],
      "source": [
        "loader = DirectoryLoader('rag-pipeline-tutorial', glob=\"**/*.md\", show_progress=True, use_multithreading=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6aNcNR8KpgY"
      },
      "outputs": [],
      "source": [
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9rn0GJLK712"
      },
      "outputs": [],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgf3dftm56cY"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "# Set the API keys\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmcvvbKVgGPU"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Initialize Pinecone client\n",
        "pc = Pinecone(api_key=os.environ.get('PINECONE_API_KEY'))\n",
        "\n",
        "# Set the name of your Pinecone Index here\n",
        "index_name = 'rag-pipeline-tutorial'\n",
        "\n",
        "# Disable deletion protection\n",
        "pc.configure_index(\n",
        "    index_name,\n",
        "    deletion_protection=\"disabled\"  # Disable protection\n",
        ")\n",
        "print(f\"Deletion protection disabled for index: {index_name}\")\n",
        "\n",
        "# Check if index exists before deleting\n",
        "existing_indexes = [index[\"name\"] for index in pc.list_indexes()]\n",
        "if index_name in existing_indexes:\n",
        "    print(f\"Deleting existing index: {index_name}\")\n",
        "    pc.delete_index(index_name)\n",
        "else:\n",
        "    print(f\"Index {index_name} does not exist, skipping delete.\")\n",
        "\n",
        "# Create the Pinecone index\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    # dimension=3072,\n",
        "    dimension=384,\n",
        "    metric='euclidean',\n",
        "    deletion_protection='enabled',\n",
        "    spec=ServerlessSpec(\n",
        "        cloud='aws',\n",
        "        region='us-east-1'\n",
        "    )\n",
        ")\n",
        "\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# Run a sanity check on the index\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNPaUIzQF7li"
      },
      "outputs": [],
      "source": [
        "# # Initialize embeddings and the vector store\n",
        "# embeddings = OpenAIEmbeddings(\n",
        "#     model='text-embedding-3-large'\n",
        "# )\n",
        "\n",
        "# # Split the documents into chunks\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "# split_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "# # Create a vector store for the documents using the specified embeddings\n",
        "# vectorstore = PineconeVectorStore.from_documents(split_docs, embeddings, index_name=index_name)\n",
        "\n",
        "\n",
        "# Run a sanity check on the index\n",
        "index.describe_index_stats()\n",
        "\n",
        "# Load a local embedding model (384D)\n",
        "embeddings_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Wrapper to match LangChain format\n",
        "class LocalEmbeddings:\n",
        "    def embed_documents(self, texts):\n",
        "        return embeddings_model.encode(texts, convert_to_numpy=True).tolist()\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        return embeddings_model.encode([text], convert_to_numpy=True).tolist()\n",
        "\n",
        "# Use local embeddings with Pinecone\n",
        "embeddings = LocalEmbeddings()\n",
        "\n",
        "# Create the vector store\n",
        "vectorstore = PineconeVectorStore.from_documents(split_docs, embeddings, index_name=index_name)\n",
        "\n",
        "print(\"Documents successfully stored in Pinecone!\")\n",
        "\n",
        "# Ask a query that is likely to score a hit against your corpus of text or data\n",
        "# In the of this example project, There's a blog post about vector databases\n",
        "query = \"What is a vector database?\"\n",
        "vectorstore.similarity_search(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziBCPFooQiQs"
      },
      "outputs": [],
      "source": [
        "# Convenience cell: ask additional arbitrary questions of the existing vectorstore that\n",
        "# was created in previous cells\n",
        "query = \"What are some kubernetes best practices?\"\n",
        "\n",
        "vectorstore.similarity_search(query)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
