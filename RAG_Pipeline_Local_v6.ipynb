{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "88313545",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88313545",
        "outputId": "5c8e0e87-1162-4d23-fd2e-40bfddbb34da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Running in Google Colab\n",
            "‚úÖ Hugging Face authentication successful!\n",
            "‚úÖ API keys loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ‚úÖ Detect environment (Google Colab vs Local)\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Check if running in Colab\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"‚úÖ Running in Google Colab\")\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "else:\n",
        "    print(\"‚úÖ Running in Local Environment\")\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()  # Load API keys from .env file\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Ensure API keys are set\n",
        "assert PINECONE_API_KEY, \"‚ùå Pinecone API Key is missing! Set it in .env for local or Colab secrets.\"\n",
        "assert OPENAI_API_KEY, \"‚ùå OpenAI API Key is missing! Set it in .env for local or Colab secrets.\"\n",
        "\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "# ‚úÖ Authenticate with Hugging Face Hub if a token is available\n",
        "if HF_TOKEN:\n",
        "    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    print(\"‚úÖ Hugging Face authentication successful!\")\n",
        "else:\n",
        "    warnings.warn(\n",
        "        \"‚ö†Ô∏è The secret `HF_TOKEN` is not set. To authenticate with Hugging Face Hub, \"\n",
        "        \"create a token at https://huggingface.co/settings/tokens and set it in Colab secrets or a .env file.\"\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ API keys loaded successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "92d21741",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92d21741",
        "outputId": "bc1d34d0-4561-4b2f-c189-2ec92b3ee843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All dependencies are installed!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ‚úÖ Install required packages (only if missing)\n",
        "try:\n",
        "    import pinecone\n",
        "    import langchain_pinecone\n",
        "    import langchain\n",
        "    import sentence_transformers\n",
        "    import openai\n",
        "except ImportError:\n",
        "    print(\"üîÑ Installing required packages...\")\n",
        "    !pip install --upgrade pinecone-client langchain-pinecone langchain sentence-transformers openai dotenv\n",
        "\n",
        "print(\"‚úÖ All dependencies are installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46a3459a",
      "metadata": {
        "id": "46a3459a"
      },
      "source": [
        "\n",
        "<a href=\"https://colab.research.google.com/github/ruparee/rag-pipeline-tutorial-notebook/blob/main/rag-pipeline-tutorial-notebook.ipynb\" target=\"_parent\">\n",
        "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "rx4-XNIFvj-P",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rx4-XNIFvj-P",
        "outputId": "be518694-6fd6-42b8-e42a-cf8c02b57f4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.11/dist-packages (5.0.1)\n",
            "Requirement already satisfied: langchain-pinecone in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Collecting langchain-pinecone\n",
            "  Using cached langchain_pinecone-0.2.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.18)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.63.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2025.1.31)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (1.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.3.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain-pinecone) (0.3.35)\n",
            "Collecting pinecone<6.0.0,>=5.4.0 (from langchain-pinecone)\n",
            "  Using cached pinecone-5.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting aiohttp<3.11,>=3.10 (from langchain-pinecone)\n",
            "  Using cached aiohttp-3.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain-pinecone) (1.26.4)\n",
            "Collecting langchain-tests<1.0.0,>=0.3.7 (from langchain-pinecone)\n",
            "  Using cached langchain_tests-0.3.12-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain-pinecone) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain-pinecone) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain-pinecone) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain-pinecone) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain-pinecone) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain-pinecone) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (1.33)\n",
            "Requirement already satisfied: pytest<9,>=7 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (8.3.4)\n",
            "Collecting pytest-asyncio<1,>=0.20 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Using cached pytest_asyncio-0.25.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting syrupy<5,>=4 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Using cached syrupy-4.8.1-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting pytest-socket<1,>=0.6.0 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Using cached pytest_socket-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "INFO: pip is looking at multiple versions of pinecone to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pinecone<6.0.0,>=5.4.0 (from langchain-pinecone)\n",
            "  Using cached pinecone-5.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Using cached pinecone-5.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting langchain-pinecone\n",
            "  Using cached langchain_pinecone-0.2.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached langchain_pinecone-0.2.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: aiohttp<3.10,>=3.9.5 in /usr/local/lib/python3.11/dist-packages (from langchain-pinecone) (3.9.5)\n",
            "INFO: pip is still looking at multiple versions of pinecone to determine which version is compatible with other requirements. This could take a while.\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (3.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<3.11,>=3.10->langchain-pinecone) (0.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Ensure all required packages are installed\n",
        "!pip install --upgrade pinecone-client langchain-pinecone langchain sentence-transformers openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "592b98ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "592b98ba",
        "outputId": "91220c40-dffd-404e-d61f-6382ea8cfbe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Directory 'rag-pipeline-tutorial' already exists. Checking for updates...\n",
            "‚úÖ Repository updated successfully!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "repo_url = \"https://github.com/ruparee/rag-pipeline-tutorial.git\"\n",
        "repo_name = \"rag-pipeline-tutorial\"\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(repo_name):\n",
        "    print(f\"‚úÖ Directory '{repo_name}' already exists. Checking for updates...\")\n",
        "    os.chdir(repo_name)  # Move into the repo directory\n",
        "    try:\n",
        "        subprocess.run([\"git\", \"pull\", \"origin\", \"main\"], check=True)\n",
        "        print(\"‚úÖ Repository updated successfully!\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ö†Ô∏è Error pulling latest changes: {e}. Proceeding with existing files.\")\n",
        "    os.chdir(\"..\")  # Move back to the original directory\n",
        "else:\n",
        "    print(f\"‚úÖ Cloning repository '{repo_name}'...\")\n",
        "    subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
        "    print(\"‚úÖ Repository cloned successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37c1665c",
      "metadata": {
        "id": "37c1665c"
      },
      "source": [
        "\n",
        "# üîç **RAG Pipeline with Pinecone & Sentence Transformers**\n",
        "This notebook implements a **Retrieval-Augmented Generation (RAG) pipeline** using:\n",
        "- **Google Colab's Secure Secret Management** (`userdata.get()`)\n",
        "- **Pinecone for vector storage**\n",
        "- **`sentence-transformers` for local embeddings**\n",
        "- **Fixes for API limits, mismatched dimensions, and deletion protection**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "62afeacf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62afeacf",
        "outputId": "693ae305-48f7-468d-9fb8-42cda74086ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ API keys loaded securely!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ‚úÖ Access secret keys securely in Google Colab\n",
        "from google.colab import userdata\n",
        "\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Ensure keys are set before proceeding\n",
        "assert PINECONE_API_KEY, \"Pinecone API Key is missing!\"\n",
        "assert OPENAI_API_KEY, \"OpenAI API Key is missing!\"\n",
        "\n",
        "import os\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "print(\"‚úÖ API keys loaded securely!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "387a5d70",
      "metadata": {
        "id": "387a5d70"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from pinecone import Pinecone\n",
        "\n",
        "# # ‚úÖ Initialize Pinecone client\n",
        "# pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
        "# index_name = \"rag-pipeline-tutorial\"\n",
        "\n",
        "# # ‚úÖ Disable deletion protection before recreating the index\n",
        "# try:\n",
        "#     pc.configure_index(index_name, deletion_protection=\"disabled\")\n",
        "#     print(f\"‚úÖ Deletion protection disabled for index: {index_name}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"‚ö†Ô∏è Warning: Could not disable deletion protection. Index may not exist yet. {e}\")\n",
        "\n",
        "# # ‚úÖ Delete existing index if it exists\n",
        "# existing_indexes = [index[\"name\"] for index in pc.list_indexes()]\n",
        "# if index_name in existing_indexes:\n",
        "#     pc.delete_index(index_name)\n",
        "#     print(f\"‚úÖ Index '{index_name}' deleted successfully.\")\n",
        "# else:\n",
        "#     print(f\"‚úÖ No existing index found. Proceeding to create a new one.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7aa19861",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aa19861",
        "outputId": "64016de8-e1fd-4d89-e786-5072db35480b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Index 'rag-pipeline-tutorial' already exists with correct dimension (384). Skipping deletion.\n"
          ]
        }
      ],
      "source": [
        "# # ‚úÖ Create a new Pinecone index with the correct dimension (384 for local embeddings)\n",
        "# from pinecone import ServerlessSpec\n",
        "\n",
        "# # ‚úÖ Create a new Pinecone index with the correct dimension (384 for local embeddings)\n",
        "# pc.create_index(\n",
        "#     name=index_name,\n",
        "#     dimension=384,  # Matches `all-MiniLM-L6-v2` model\n",
        "#     metric=\"euclidean\",\n",
        "#     deletion_protection=\"enabled\",  # Re-enable if needed\n",
        "#         spec=ServerlessSpec(  # Correcting the spec definition\n",
        "#         cloud=\"aws\",\n",
        "#         region=\"us-east-1\"\n",
        "#     )\n",
        "# )\n",
        "# print(f\"‚úÖ New Pinecone index '{index_name}' created with dimension 384.\")\n",
        "\n",
        "\n",
        "from pinecone import Pinecone\n",
        "\n",
        "# ‚úÖ Initialize Pinecone client\n",
        "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
        "index_name = \"rag-pipeline-tutorial\"\n",
        "\n",
        "# ‚úÖ Define expected embedding dimension\n",
        "EXPECTED_DIMENSION = 384  # Matches `all-MiniLM-L6-v2` model\n",
        "\n",
        "# ‚úÖ Check if the index exists and compare dimensions\n",
        "existing_indexes = [index[\"name\"] for index in pc.list_indexes()]\n",
        "if index_name in existing_indexes:\n",
        "    index_stats = pc.Index(index_name).describe_index_stats()\n",
        "    current_dimension = index_stats.get(\"dimension\", None)\n",
        "\n",
        "    if current_dimension == EXPECTED_DIMENSION:\n",
        "        print(f\"‚úÖ Index '{index_name}' already exists with correct dimension ({EXPECTED_DIMENSION}). Skipping deletion.\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Dimension mismatch! Expected {EXPECTED_DIMENSION}, but found {current_dimension}. Recreating index...\")\n",
        "        pc.delete_index(index_name)\n",
        "        print(f\"‚úÖ Deleted index '{index_name}'. Proceeding with recreation.\")\n",
        "else:\n",
        "    print(f\"‚úÖ No existing index found. Proceeding to create a new one.\")\n",
        "\n",
        "# ‚úÖ Create Pinecone index only if necessary\n",
        "if index_name not in existing_indexes or current_dimension != EXPECTED_DIMENSION:\n",
        "    from pinecone import ServerlessSpec\n",
        "\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=EXPECTED_DIMENSION,  # Ensure it matches the model\n",
        "        metric=\"euclidean\",\n",
        "        deletion_protection=\"enabled\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "    )\n",
        "    print(f\"‚úÖ New Pinecone index '{index_name}' created with dimension {EXPECTED_DIMENSION}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b7eec249",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7eec249",
        "outputId": "a41d0826-5697-443c-9e1a-4c22758071e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Local embeddings model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.embeddings.base import Embeddings\n",
        "\n",
        "# ‚úÖ Load a local embedding model (384D)\n",
        "embeddings_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# ‚úÖ Wrapper to ensure compatibility with LangChain\n",
        "class LocalEmbeddings(Embeddings):\n",
        "    def embed_documents(self, texts):\n",
        "        return embeddings_model.encode(texts, convert_to_numpy=True).tolist()\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        return embeddings_model.encode([text], convert_to_numpy=True).tolist()\n",
        "\n",
        "embeddings = LocalEmbeddings()\n",
        "\n",
        "print(\"‚úÖ Local embeddings model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "12918492",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12918492",
        "outputId": "2440ee5d-b04b-46ef-8e94-55dc7f47a1e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded and split 3 document chunks!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "# ‚úÖ Example documents (Replace with your actual dataset)\n",
        "docs = [\n",
        "    \"Vector databases store high-dimensional vectors used for semantic search.\",\n",
        "    \"Pinecone is a serverless vector database optimized for AI applications.\",\n",
        "    \"Large Language Models (LLMs) use vector databases to improve retrieval accuracy.\"\n",
        "]\n",
        "\n",
        "# ‚úÖ Convert docs into a list of `Document` objects\n",
        "documents = [Document(page_content=doc) for doc in docs]\n",
        "\n",
        "\n",
        "# ‚úÖ Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
        "# ‚úÖ Use the correct method to split documents\n",
        "split_docs = text_splitter.split_documents(documents)\n",
        "\n",
        "\n",
        "print(f\"‚úÖ Loaded and split {len(split_docs)} document chunks!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e41e99a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e41e99a3",
        "outputId": "4fb7a507-896f-4c44-fcba-830654bc8bd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Documents successfully stored in Pinecone!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ‚úÖ Store document vectors in Pinecone\n",
        "vectorstore = PineconeVectorStore.from_documents(split_docs, embeddings, index_name=index_name)\n",
        "print(\"‚úÖ Documents successfully stored in Pinecone!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1e213947",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e213947",
        "outputId": "b7499497-6c2e-4a5c-d634-e75758f2bd7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Index is ready with 9 vectors.\n",
            "‚úÖ Running similarity search...\n",
            "Result 1: Vector databases store high-dimensional vectors used for semantic search.\n",
            "Result 2: Vector databases store high-dimensional vectors used for semantic search.\n",
            "Result 3: Vector databases store high-dimensional vectors used for semantic search.\n",
            "Result 4: Large Language Models (LLMs) use vector databases to improve retrieval accuracy.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# # ‚úÖ Run a similarity search query\n",
        "# query = \"What is a vector database?\"\n",
        "# results = vectorstore.similarity_search(query)\n",
        "\n",
        "# # ‚úÖ Print retrieved results\n",
        "# for i, doc in enumerate(results):\n",
        "#     print(f\"Result {i+1}: {doc.page_content}\")\n",
        "\n",
        "# ######### THIS WORKS\n",
        "\n",
        "# import time\n",
        "\n",
        "# print(\"‚è≥ Waiting for Pinecone index to be ready...\")\n",
        "# time.sleep(20)  # Allow Pinecone to fully initialize\n",
        "\n",
        "# print(\"‚úÖ Running similarity search...\")\n",
        "# query = \"What is a vector database?\"\n",
        "# results = vectorstore.similarity_search(query)\n",
        "\n",
        "# for i, doc in enumerate(results):\n",
        "#     print(f\"Result {i+1}: {doc.page_content}\")\n",
        "\n",
        "\n",
        "#  ######################### WORKS TOO\n",
        "# import time\n",
        "\n",
        "# # Ensure index is populated before querying\n",
        "# index_stats = vectorstore._index.describe_index_stats()\n",
        "# total_vectors = index_stats.get(\"total_vector_count\", 0)\n",
        "\n",
        "# if total_vectors == 0:\n",
        "#     print(\"‚è≥ No vectors found in index. Waiting for data to be indexed...\")\n",
        "#     time.sleep(20)  # Give Pinecone time to index the data\n",
        "\n",
        "# print(\"‚úÖ Running similarity search...\")\n",
        "# query = \"What is a vector database?\"\n",
        "# results = vectorstore.similarity_search(query)\n",
        "\n",
        "# # Print results\n",
        "# if results:\n",
        "#     for i, doc in enumerate(results):\n",
        "#         print(f\"Result {i+1}: {doc.page_content}\")\n",
        "# else:\n",
        "#     print(\"‚ö†Ô∏è No results found! Try running the search again.\")\n",
        "\n",
        "\n",
        "# #### WORKS  AS WELL\n",
        "# import time\n",
        "\n",
        "# # ‚úÖ Function to wait for Pinecone index readiness with retries\n",
        "# def wait_for_index(vectorstore, max_retries=4, wait_time=5):\n",
        "#     for attempt in range(max_retries):\n",
        "#         index_stats = vectorstore._index.describe_index_stats()\n",
        "#         total_vectors = index_stats.get(\"total_vector_count\", 0)\n",
        "\n",
        "#         if total_vectors > 0:\n",
        "#             print(f\"‚úÖ Index is ready with {total_vectors} vectors.\")\n",
        "#             return\n",
        "\n",
        "#         print(f\"‚è≥ No vectors found (Attempt {attempt+1}/{max_retries}). Waiting {wait_time}s before retrying...\")\n",
        "#         time.sleep(wait_time)\n",
        "\n",
        "#     print(\"‚ö†Ô∏è Index is still empty after retries. Proceeding, but results may be incomplete.\")\n",
        "\n",
        "# # ‚úÖ Ensure index is populated before querying with retry logic\n",
        "# wait_for_index(vectorstore, max_retries=4, wait_time=5)\n",
        "\n",
        "# print(\"‚úÖ Running similarity search...\")\n",
        "# query = \"What is a vector database?\"\n",
        "# results = vectorstore.similarity_search(query)\n",
        "\n",
        "# # ‚úÖ Print results\n",
        "# if results:\n",
        "#     for i, doc in enumerate(results):\n",
        "#         print(f\"Result {i+1}: {doc.page_content}\")\n",
        "# else:\n",
        "#     print(\"‚ö†Ô∏è No results found! Try running the search again.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Updated Code with Exponential Backoff\n",
        "import time\n",
        "\n",
        "# ‚úÖ Function to wait for Pinecone index readiness with exponential backoff\n",
        "def wait_for_index(vectorstore, max_retries=5, initial_wait=5, max_wait=60):\n",
        "    wait_time = initial_wait  # Start with 5 seconds\n",
        "    total_wait = 0  # Track total wait time\n",
        "\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        index_stats = vectorstore._index.describe_index_stats()\n",
        "        total_vectors = index_stats.get(\"total_vector_count\", 0)\n",
        "\n",
        "        if total_vectors > 0:\n",
        "            print(f\"‚úÖ Index is ready with {total_vectors} vectors.\")\n",
        "            return True  # Exit function early\n",
        "\n",
        "        print(f\"‚è≥ No vectors found (Attempt {attempt}/{max_retries}). Waiting {wait_time}s before retrying...\")\n",
        "        time.sleep(wait_time)\n",
        "\n",
        "        # Double wait time for next attempt, but cap at max_wait\n",
        "        total_wait += wait_time\n",
        "        wait_time = min(wait_time * 2, max_wait)\n",
        "\n",
        "        if total_wait >= max_wait:\n",
        "            break  # Stop retrying if we've waited too long\n",
        "\n",
        "    print(\"‚ö†Ô∏è Index is still empty after retries. Proceeding, but results may be incomplete.\")\n",
        "    return False  # Indicate that retries did not succeed\n",
        "\n",
        "# ‚úÖ Ensure index is populated before querying with retry logic\n",
        "wait_for_index(vectorstore, max_retries=5, initial_wait=5, max_wait=60)\n",
        "\n",
        "print(\"‚úÖ Running similarity search...\")\n",
        "query = \"What is a vector database?\"\n",
        "results = vectorstore.similarity_search(query)\n",
        "\n",
        "# ‚úÖ Print results\n",
        "if results:\n",
        "    for i, doc in enumerate(results):\n",
        "        print(f\"Result {i+1}: {doc.page_content}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No results found! Try running the search again.\")\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b03a3c70",
      "metadata": {
        "id": "b03a3c70"
      },
      "source": [
        "\n",
        "## üöÄ **Next Enhancements**\n",
        "1. **Improve retrieval quality** ‚Äì Fine-tune embeddings for domain-specific knowledge.\n",
        "2. **Optimize query performance** ‚Äì Implement vector caching strategies.\n",
        "3. **Enhance batch processing** ‚Äì Improve bulk vector updates in Pinecone.\n",
        "4. **Implement Hybrid Search** ‚Äì Combine **Vector + Keyword Search** for better accuracy.\n",
        "5. **Use Re-Ranking models** ‚Äì Apply `cross-encoder` to improve ranking.\n",
        "6. **Expand Data Sources** ‚Äì Integrate a more diverse document set.\n",
        "7. **Integrate a Chatbot** ‚Äì Build an AI chatbot using the Pinecone knowledge base.\n",
        "\n",
        "üîπ This notebook **fully integrates fixes for API limits, mismatched dimensions, deletion protection, and retrieval optimizations**.  \n",
        "üí° Feel free to experiment and extend the pipeline with the listed enhancements! üéØ  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "uxWSbWaYz_C8",
      "metadata": {
        "id": "uxWSbWaYz_C8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}