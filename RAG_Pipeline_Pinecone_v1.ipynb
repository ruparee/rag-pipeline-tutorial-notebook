{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a3459a",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "\n",
    "<a href=\"https://colab.research.google.com/github/ruparee/rag-pipeline-tutorial-notebook/blob/main/rag-pipeline-tutorial-notebook.ipynb\" target=\"_parent\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c1665c",
   "metadata": {},
   "source": [
    "\n",
    "# üîç **RAG Pipeline with Pinecone & Sentence Transformers**\n",
    "This notebook implements a **Retrieval-Augmented Generation (RAG) pipeline** using:\n",
    "- **Google Colab's Secure Secret Management** (`userdata.get()`)\n",
    "- **Pinecone for vector storage**\n",
    "- **`sentence-transformers` for local embeddings**\n",
    "- **Fixes for API limits, mismatched dimensions, and deletion protection**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62afeacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚úÖ Access secret keys securely in Google Colab\n",
    "from google.colab import userdata\n",
    "\n",
    "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
    "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Ensure keys are set before proceeding\n",
    "assert PINECONE_API_KEY, \"Pinecone API Key is missing!\"\n",
    "assert OPENAI_API_KEY, \"OpenAI API Key is missing!\"\n",
    "\n",
    "import os\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "print(\"‚úÖ API keys loaded securely!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a5d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# ‚úÖ Initialize Pinecone client\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "index_name = \"rag-pipeline-tutorial\"\n",
    "\n",
    "# ‚úÖ Disable deletion protection before recreating the index\n",
    "try:\n",
    "    pc.configure_index(index_name, deletion_protection=\"disabled\")\n",
    "    print(f\"‚úÖ Deletion protection disabled for index: {index_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Warning: Could not disable deletion protection. Index may not exist yet. {e}\")\n",
    "\n",
    "# ‚úÖ Delete existing index if it exists\n",
    "existing_indexes = [index[\"name\"] for index in pc.list_indexes()]\n",
    "if index_name in existing_indexes:\n",
    "    pc.delete_index(index_name)\n",
    "    print(f\"‚úÖ Index '{index_name}' deleted successfully.\")\n",
    "else:\n",
    "    print(f\"‚úÖ No existing index found. Proceeding to create a new one.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa19861",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚úÖ Create a new Pinecone index with the correct dimension (384 for local embeddings)\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384,  # Matches `all-MiniLM-L6-v2` model\n",
    "    metric=\"euclidean\",\n",
    "    deletion_protection=\"enabled\",  # Re-enable if needed\n",
    "    spec={\"cloud\": \"aws\", \"region\": \"us-east-1\"}\n",
    ")\n",
    "print(f\"‚úÖ New Pinecone index '{index_name}' created with dimension 384.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eec249",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "# ‚úÖ Load a local embedding model (384D)\n",
    "embeddings_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ‚úÖ Wrapper to ensure compatibility with LangChain\n",
    "class LocalEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts):\n",
    "        return embeddings_model.encode(texts, convert_to_numpy=True).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return embeddings_model.encode([text], convert_to_numpy=True).tolist()\n",
    "\n",
    "embeddings = LocalEmbeddings()\n",
    "\n",
    "print(\"‚úÖ Local embeddings model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12918492",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ‚úÖ Example documents (Replace with your actual dataset)\n",
    "docs = [\n",
    "    \"Vector databases store high-dimensional vectors used for semantic search.\",\n",
    "    \"Pinecone is a serverless vector database optimized for AI applications.\",\n",
    "    \"Large Language Models (LLMs) use vector databases to improve retrieval accuracy.\"\n",
    "]\n",
    "\n",
    "# ‚úÖ Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "split_docs = text_splitter.split_text(docs)\n",
    "\n",
    "print(f\"‚úÖ Loaded and split {len(split_docs)} document chunks!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e99a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚úÖ Store document vectors in Pinecone\n",
    "vectorstore = PineconeVectorStore.from_documents(split_docs, embeddings, index_name=index_name)\n",
    "print(\"‚úÖ Documents successfully stored in Pinecone!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e213947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚úÖ Run a similarity search query\n",
    "query = \"What is a vector database?\"\n",
    "results = vectorstore.similarity_search(query)\n",
    "\n",
    "# ‚úÖ Print retrieved results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"Result {i+1}: {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03a3c70",
   "metadata": {},
   "source": [
    "\n",
    "## üöÄ **Next Enhancements**\n",
    "1. **Improve retrieval quality** ‚Äì Fine-tune embeddings for domain-specific knowledge.\n",
    "2. **Optimize query performance** ‚Äì Implement vector caching strategies.\n",
    "3. **Enhance batch processing** ‚Äì Improve bulk vector updates in Pinecone.\n",
    "4. **Implement Hybrid Search** ‚Äì Combine **Vector + Keyword Search** for better accuracy.\n",
    "5. **Use Re-Ranking models** ‚Äì Apply `cross-encoder` to improve ranking.\n",
    "6. **Expand Data Sources** ‚Äì Integrate a more diverse document set.\n",
    "7. **Integrate a Chatbot** ‚Äì Build an AI chatbot using the Pinecone knowledge base.\n",
    "\n",
    "üîπ This notebook **fully integrates fixes for API limits, mismatched dimensions, deletion protection, and retrieval optimizations**.  \n",
    "üí° Feel free to experiment and extend the pipeline with the listed enhancements! üéØ  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
