{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "88313545",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88313545",
        "outputId": "1ff6d732-9153-4ba8-fffd-3e6b38e4625a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Running in Google Colab\n",
            "‚úÖ API keys loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ‚úÖ Detect environment (Google Colab vs Local)\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Check if running in Colab\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"‚úÖ Running in Google Colab\")\n",
        "    from google.colab import userdata\n",
        "    PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "else:\n",
        "    print(\"‚úÖ Running in Local Environment\")\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()  # Load API keys from .env file\n",
        "    PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Ensure API keys are set\n",
        "assert PINECONE_API_KEY, \"‚ùå Pinecone API Key is missing! Set it in .env for local or Colab secrets.\"\n",
        "assert OPENAI_API_KEY, \"‚ùå OpenAI API Key is missing! Set it in .env for local or Colab secrets.\"\n",
        "\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "print(\"‚úÖ API keys loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "92d21741",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92d21741",
        "outputId": "5d9c7245-f865-4445-fa07-7af831f3200c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All dependencies are installed!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ‚úÖ Install required packages (only if missing)\n",
        "try:\n",
        "    import pinecone\n",
        "    import langchain_pinecone\n",
        "    import langchain\n",
        "    import sentence_transformers\n",
        "    import openai\n",
        "except ImportError:\n",
        "    print(\"üîÑ Installing required packages...\")\n",
        "    !pip install --upgrade pinecone-client langchain-pinecone langchain sentence-transformers openai dotenv\n",
        "\n",
        "print(\"‚úÖ All dependencies are installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46a3459a",
      "metadata": {
        "id": "46a3459a"
      },
      "source": [
        "\n",
        "<a href=\"https://colab.research.google.com/github/ruparee/rag-pipeline-tutorial-notebook/blob/main/rag-pipeline-tutorial-notebook.ipynb\" target=\"_parent\">\n",
        "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "rx4-XNIFvj-P",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rx4-XNIFvj-P",
        "outputId": "601b6f14-50af-42a2-9dac-87372ef81e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.11/dist-packages (5.0.1)\n",
            "Requirement already satisfied: langchain-pinecone in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Collecting langchain-pinecone\n",
            "  Using cached langchain_pinecone-0.2.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.18)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.63.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2025.1.31)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (1.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.3.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain-pinecone) (0.3.35)\n",
            "Collecting pinecone<6.0.0,>=5.4.0 (from langchain-pinecone)\n",
            "  Using cached pinecone-5.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting aiohttp<3.11,>=3.10 (from langchain-pinecone)\n",
            "  Using cached aiohttp-3.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain-pinecone) (1.26.4)\n",
            "Collecting langchain-tests<1.0.0,>=0.3.7 (from langchain-pinecone)\n",
            "  Using cached langchain_tests-0.3.12-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain-pinecone) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain-pinecone) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain-pinecone) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain-pinecone) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain-pinecone) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain-pinecone) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (1.33)\n",
            "Requirement already satisfied: pytest<9,>=7 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (8.3.4)\n",
            "Collecting pytest-asyncio<1,>=0.20 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Using cached pytest_asyncio-0.25.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting syrupy<5,>=4 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Using cached syrupy-4.8.1-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting pytest-socket<1,>=0.6.0 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Using cached pytest_socket-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "INFO: pip is looking at multiple versions of pinecone to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pinecone<6.0.0,>=5.4.0 (from langchain-pinecone)\n",
            "  Using cached pinecone-5.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Using cached pinecone-5.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting langchain-pinecone\n",
            "  Using cached langchain_pinecone-0.2.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached langchain_pinecone-0.2.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: aiohttp<3.10,>=3.9.5 in /usr/local/lib/python3.11/dist-packages (from langchain-pinecone) (3.9.5)\n",
            "INFO: pip is still looking at multiple versions of pinecone to determine which version is compatible with other requirements. This could take a while.\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (3.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<3.11,>=3.10->langchain-pinecone) (0.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Ensure all required packages are installed\n",
        "!pip install --upgrade pinecone-client langchain-pinecone langchain sentence-transformers openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "592b98ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "592b98ba",
        "outputId": "11127196-a61c-482c-eb1a-fb61a2e11e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Directory 'rag-pipeline-tutorial' already exists. Checking for updates...\n",
            "‚úÖ Repository updated successfully!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "repo_url = \"https://github.com/ruparee/rag-pipeline-tutorial.git\"\n",
        "repo_name = \"rag-pipeline-tutorial\"\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(repo_name):\n",
        "    print(f\"‚úÖ Directory '{repo_name}' already exists. Checking for updates...\")\n",
        "    os.chdir(repo_name)  # Move into the repo directory\n",
        "    try:\n",
        "        subprocess.run([\"git\", \"pull\", \"origin\", \"main\"], check=True)\n",
        "        print(\"‚úÖ Repository updated successfully!\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ö†Ô∏è Error pulling latest changes: {e}. Proceeding with existing files.\")\n",
        "    os.chdir(\"..\")  # Move back to the original directory\n",
        "else:\n",
        "    print(f\"‚úÖ Cloning repository '{repo_name}'...\")\n",
        "    subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
        "    print(\"‚úÖ Repository cloned successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37c1665c",
      "metadata": {
        "id": "37c1665c"
      },
      "source": [
        "\n",
        "# üîç **RAG Pipeline with Pinecone & Sentence Transformers**\n",
        "This notebook implements a **Retrieval-Augmented Generation (RAG) pipeline** using:\n",
        "- **Google Colab's Secure Secret Management** (`userdata.get()`)\n",
        "- **Pinecone for vector storage**\n",
        "- **`sentence-transformers` for local embeddings**\n",
        "- **Fixes for API limits, mismatched dimensions, and deletion protection**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "62afeacf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62afeacf",
        "outputId": "63f8c6b8-1294-4de5-8046-aed4f849ab23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ API keys loaded securely!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ‚úÖ Access secret keys securely in Google Colab\n",
        "from google.colab import userdata\n",
        "\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Ensure keys are set before proceeding\n",
        "assert PINECONE_API_KEY, \"Pinecone API Key is missing!\"\n",
        "assert OPENAI_API_KEY, \"OpenAI API Key is missing!\"\n",
        "\n",
        "import os\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "print(\"‚úÖ API keys loaded securely!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "387a5d70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "387a5d70",
        "outputId": "858b26f4-4483-428e-e269-85a603ecde9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Deletion protection disabled for index: rag-pipeline-tutorial\n",
            "‚úÖ Index 'rag-pipeline-tutorial' deleted successfully.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pinecone import Pinecone\n",
        "\n",
        "# ‚úÖ Initialize Pinecone client\n",
        "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
        "index_name = \"rag-pipeline-tutorial\"\n",
        "\n",
        "# ‚úÖ Disable deletion protection before recreating the index\n",
        "try:\n",
        "    pc.configure_index(index_name, deletion_protection=\"disabled\")\n",
        "    print(f\"‚úÖ Deletion protection disabled for index: {index_name}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Warning: Could not disable deletion protection. Index may not exist yet. {e}\")\n",
        "\n",
        "# ‚úÖ Delete existing index if it exists\n",
        "existing_indexes = [index[\"name\"] for index in pc.list_indexes()]\n",
        "if index_name in existing_indexes:\n",
        "    pc.delete_index(index_name)\n",
        "    print(f\"‚úÖ Index '{index_name}' deleted successfully.\")\n",
        "else:\n",
        "    print(f\"‚úÖ No existing index found. Proceeding to create a new one.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7aa19861",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aa19861",
        "outputId": "4dc1ee74-bb5b-4f6b-8a5d-3b69f30680e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ New Pinecone index 'rag-pipeline-tutorial' created with dimension 384.\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Create a new Pinecone index with the correct dimension (384 for local embeddings)\n",
        "from pinecone import ServerlessSpec\n",
        "\n",
        "# ‚úÖ Create a new Pinecone index with the correct dimension (384 for local embeddings)\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=384,  # Matches `all-MiniLM-L6-v2` model\n",
        "    metric=\"euclidean\",\n",
        "    deletion_protection=\"enabled\",  # Re-enable if needed\n",
        "        spec=ServerlessSpec(  # Correcting the spec definition\n",
        "        cloud=\"aws\",\n",
        "        region=\"us-east-1\"\n",
        "    )\n",
        ")\n",
        "print(f\"‚úÖ New Pinecone index '{index_name}' created with dimension 384.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b7eec249",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7eec249",
        "outputId": "98d731e1-acd9-4e6b-8db6-597494d518da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Local embeddings model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.embeddings.base import Embeddings\n",
        "\n",
        "# ‚úÖ Load a local embedding model (384D)\n",
        "embeddings_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# ‚úÖ Wrapper to ensure compatibility with LangChain\n",
        "class LocalEmbeddings(Embeddings):\n",
        "    def embed_documents(self, texts):\n",
        "        return embeddings_model.encode(texts, convert_to_numpy=True).tolist()\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        return embeddings_model.encode([text], convert_to_numpy=True).tolist()\n",
        "\n",
        "embeddings = LocalEmbeddings()\n",
        "\n",
        "print(\"‚úÖ Local embeddings model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "12918492",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12918492",
        "outputId": "93f4b30a-13f4-45e7-c128-c5805021ef9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded and split 3 document chunks!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "# ‚úÖ Example documents (Replace with your actual dataset)\n",
        "docs = [\n",
        "    \"Vector databases store high-dimensional vectors used for semantic search.\",\n",
        "    \"Pinecone is a serverless vector database optimized for AI applications.\",\n",
        "    \"Large Language Models (LLMs) use vector databases to improve retrieval accuracy.\"\n",
        "]\n",
        "\n",
        "# ‚úÖ Convert docs into a list of `Document` objects\n",
        "documents = [Document(page_content=doc) for doc in docs]\n",
        "\n",
        "\n",
        "# ‚úÖ Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
        "# ‚úÖ Use the correct method to split documents\n",
        "split_docs = text_splitter.split_documents(documents)\n",
        "\n",
        "\n",
        "print(f\"‚úÖ Loaded and split {len(split_docs)} document chunks!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e41e99a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e41e99a3",
        "outputId": "643e45ec-f33f-4e0f-9c8e-b9928ffd8e98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Documents successfully stored in Pinecone!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ‚úÖ Store document vectors in Pinecone\n",
        "vectorstore = PineconeVectorStore.from_documents(split_docs, embeddings, index_name=index_name)\n",
        "print(\"‚úÖ Documents successfully stored in Pinecone!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1e213947",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e213947",
        "outputId": "220db9b2-416b-4ab9-f9f9-17a430b3533a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≥ No vectors found (Attempt 1/5). Waiting 5s before retrying...\n",
            "‚è≥ No vectors found (Attempt 2/5). Waiting 10s before retrying...\n",
            "‚úÖ Index is ready with 3 vectors.\n",
            "‚úÖ Running similarity search...\n",
            "Result 1: Vector databases store high-dimensional vectors used for semantic search.\n",
            "Result 2: Large Language Models (LLMs) use vector databases to improve retrieval accuracy.\n",
            "Result 3: Pinecone is a serverless vector database optimized for AI applications.\n"
          ]
        }
      ],
      "source": [
        "# Updated Code with Exponential Backoff\n",
        "import time\n",
        "\n",
        "# ‚úÖ Function to wait for Pinecone index readiness with exponential backoff\n",
        "def wait_for_index(vectorstore, max_retries=5, initial_wait=5, max_wait=60):\n",
        "    wait_time = initial_wait  # Start with 5 seconds\n",
        "    total_wait = 0  # Track total wait time\n",
        "\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        index_stats = vectorstore._index.describe_index_stats()\n",
        "        total_vectors = index_stats.get(\"total_vector_count\", 0)\n",
        "\n",
        "        if total_vectors > 0:\n",
        "            print(f\"‚úÖ Index is ready with {total_vectors} vectors.\")\n",
        "            return True  # Exit function early\n",
        "\n",
        "        print(f\"‚è≥ No vectors found (Attempt {attempt}/{max_retries}). Waiting {wait_time}s before retrying...\")\n",
        "        time.sleep(wait_time)\n",
        "\n",
        "        # Double wait time for next attempt, but cap at max_wait\n",
        "        total_wait += wait_time\n",
        "        wait_time = min(wait_time * 2, max_wait)\n",
        "\n",
        "        if total_wait >= max_wait:\n",
        "            break  # Stop retrying if we've waited too long\n",
        "\n",
        "    print(\"‚ö†Ô∏è Index is still empty after retries. Proceeding, but results may be incomplete.\")\n",
        "    return False  # Indicate that retries did not succeed\n",
        "\n",
        "# ‚úÖ Ensure index is populated before querying with retry logic\n",
        "wait_for_index(vectorstore, max_retries=5, initial_wait=5, max_wait=60)\n",
        "\n",
        "print(\"‚úÖ Running similarity search...\")\n",
        "query = \"What is a vector database?\"\n",
        "results = vectorstore.similarity_search(query)\n",
        "\n",
        "# ‚úÖ Print results\n",
        "if results:\n",
        "    for i, doc in enumerate(results):\n",
        "        print(f\"Result {i+1}: {doc.page_content}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No results found! Try running the search again.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b03a3c70",
      "metadata": {
        "id": "b03a3c70"
      },
      "source": [
        "\n",
        "## üöÄ **Next Enhancements**\n",
        "1. **Improve retrieval quality** ‚Äì Fine-tune embeddings for domain-specific knowledge.\n",
        "2. **Optimize query performance** ‚Äì Implement vector caching strategies.\n",
        "3. **Enhance batch processing** ‚Äì Improve bulk vector updates in Pinecone.\n",
        "4. **Implement Hybrid Search** ‚Äì Combine **Vector + Keyword Search** for better accuracy.\n",
        "5. **Use Re-Ranking models** ‚Äì Apply `cross-encoder` to improve ranking.\n",
        "6. **Expand Data Sources** ‚Äì Integrate a more diverse document set.\n",
        "7. **Integrate a Chatbot** ‚Äì Build an AI chatbot using the Pinecone knowledge base.\n",
        "\n",
        "üîπ This notebook **fully integrates fixes for API limits, mismatched dimensions, deletion protection, and retrieval optimizations**.  \n",
        "üí° Feel free to experiment and extend the pipeline with the listed enhancements! üéØ  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "uxWSbWaYz_C8",
      "metadata": {
        "id": "uxWSbWaYz_C8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
